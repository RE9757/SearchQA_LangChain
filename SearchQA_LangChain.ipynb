{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üîç Search-Augmented Question Answering with SerpAPI + LangChain\n",
        "\n",
        "## 1. Introduction\n",
        "- Brief description of the goal: using search engine results to build a lightweight Q&A system\n",
        "- Tech stack: Python, SerpAPI, LangChain, HuggingFace, FAISS\n",
        "\n",
        "## 2. Web Search and Text Collection (via SerpAPI)\n",
        "- Use SerpAPI to query a topic (e.g., ‚ÄúHistory of AI‚Äù)\n",
        "- Extract top result URLs and article content\n",
        "- Save all content to `result.txt`"
      ],
      "metadata": {
        "id": "CBhnJKlZLsMf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0S9hIlY5Ji2Q"
      },
      "outputs": [],
      "source": [
        "!pip install newspaper3k\n",
        "!pip install langchain faiss-cpu\n",
        "!pip install sentence-transformers\n",
        "!pip install transformers serpapi\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from newspaper import Article\n",
        "\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def serpapi_bing_search(query, api_key, num_results=10):\n",
        "    url = \"https://serpapi.com/search\"\n",
        "    params = {\n",
        "        \"engine\": \"bing\",            # Using Bing Engine\n",
        "        \"q\": query,                  # Search for keyword\n",
        "        \"count\": num_results,        # result\n",
        "        \"api_key\": api_key\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        res = requests.get(url, params=params)\n",
        "        res.raise_for_status()\n",
        "        data = res.json()\n",
        "\n",
        "        results = data.get(\"organic_results\", [])\n",
        "        urls = [item.get(\"link\") for item in results if \"link\" in item]\n",
        "        return urls\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return []\n",
        "\n",
        "def detailed_search(url, txtNum=500):\n",
        "    article = Article(url)\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    print(article.title)\n",
        "    print(article.text[:txtNum])\n",
        "\n",
        "def save_articles_to_file(urls, output_path=\"result.txt\"):\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for url in urls:\n",
        "            try:\n",
        "                article = Article(url)\n",
        "                article.download()\n",
        "                article.parse()\n",
        "                f.write(f\"=== {article.title} ===\\n\")\n",
        "                f.write(article.text + \"\\n\\n\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to process {url}: {e}\")\n"
      ],
      "metadata": {
        "id": "79HF9rCjLyyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Text Processing and Vector Embedding\n",
        "- Load `result.txt`\n",
        "- Split into chunks\n",
        "- Use `sentence-transformers` + FAISS for vector indexing"
      ],
      "metadata": {
        "id": "e7JbqVVkRTQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keyword = input(\"Enter search keyword: \")\n",
        "os.environ[\"SERPAPI_API_KEY\"] = input(\"Enter your SerpAPI key: \")\n",
        "api_key = os.environ[\"SERPAPI_API_KEY\"]\n",
        "\n",
        "urls = serpapi_bing_search(keyword, api_key)\n",
        "save_articles_to_file(urls)"
      ],
      "metadata": {
        "id": "7dz9IOwQQn9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Lightweight LLM + QA Pipeline\n",
        "- Load a tiny LLM using HuggingFace `pipeline`\n",
        "- Setup `RetrievalQA` using LangChain\n",
        "- Ask sample questions and print answers"
      ],
      "metadata": {
        "id": "9guk9sSFRUhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using tiny model\n",
        "generator = pipeline(\"text-generation\", model=\"sshleifer/tiny-gpt2\", max_new_tokens=256)\n",
        "llm = HuggingFacePipeline(pipeline=generator)\n",
        "\n",
        "# Loading the privous result\n",
        "loader = TextLoader(\"result.txt\", encoding=\"utf-8\")\n",
        "docs = loader.load()\n",
        "\n",
        "# slice the content\n",
        "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "split_docs = text_splitter.split_documents(docs)\n",
        "\n",
        "# 3. Text vectorizationÔºàusing English modelÔºâ\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "db = FAISS.from_documents(split_docs, embedding)\n",
        "\n",
        "# 4. QA system setup\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever())\n",
        "\n",
        "# 5. test QA\n",
        "question = \"\"\n",
        "result = qa.run(question)\n",
        "print(\"Answer:\", result)"
      ],
      "metadata": {
        "id": "KoFsV2nTRJ2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Notes on API Key Security\n",
        "- Use environment variable or mount secrets in Colab to protect SerpAPI key"
      ],
      "metadata": {
        "id": "BtR8atktY_i6"
      }
    }
  ]
}